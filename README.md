# Artificial-Neural-Network

Artificial Neural Network, all fully connected layers, including forward propagation and backpropagation functions.

This file is for MNIST dataset.

Using Relu function as activition function, which can be modified in activaion_func and activation_fcnp.

The layers are set in the list using the unit number such as [784,64,64,10]. The first is input layer, the last is output layer. Softmax and cross entropy is used.

Pay attention to the Path you'll be use.
